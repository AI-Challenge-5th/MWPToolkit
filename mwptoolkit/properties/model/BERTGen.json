{
    "pretrained_model_path":"pretrain/gpt2_cn",
    "embedding_size": 128,
    "ffn_size": 1024,
    "num_encoder_layers": 2,
    "num_decoder_layers": 2,
    "num_heads": 8,
    "attn_dropout_ratio": 0.5,
    "attn_weight_dropout_ratio": 0.5,
    "ffn_dropout_ratio": 0.5,
    "embedding_dropout_ratio":0.5,
    "learning_rate": 0.001,
    "warmup_steps": 1500,
    "beam_size": 5,
    "decoding_strategy": "greedy_search",
    "share_vocab":true,
    "symbol_for_tree":false,
    "max_len":128,
    "epoch_nums":200,
    "teacher_force_ratio":0.8,
    "add_sos":true,
    "add_eos":true
}